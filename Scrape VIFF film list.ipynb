{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape VIFF films to Excel\n",
    "\n",
    "Script to scrape film information from the Vancouver International Film Festival (VIFF) website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.7.15'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from platform import python_version\n",
    "python_version()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParseResult(scheme='https', netloc='www.viff.org', path='/Online/default.asp', params='', query='doWork::WScontent::loadArticle=Load&BOparam::WScontent::loadArticle::article_id=D5FA11B0-61FD-4217-AAF7-1FC44D897DA1', fragment='')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Python script to scrape film information from the\n",
    "# Vancouver International Film Festival (VIFF) website\n",
    "# using lxml\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import pickle\n",
    "from lxml import html\n",
    "\n",
    "try:\n",
    "    # For Python 3\n",
    "    from urllib.parse import urlparse\n",
    "except ImportError:\n",
    "    # For Python 2\n",
    "    from urlparse import urlparse\n",
    "\n",
    "# build the lxml tree from the chosen website\n",
    "# Used this as guide:\n",
    "# https://docs.python-guide.org/scenarios/scrape/\n",
    "\n",
    "# Enter the base URL where the A-Z list of films is here (from\n",
    "# the viff.org home page find the 'Search by Title' option):\n",
    "start_page = \"https://www.viff.org/Online/default.asp?\" \\\n",
    "             \"doWork::WScontent::loadArticle=Load&BOparam::\" \\\n",
    "             \"WScontent::loadArticle::\" \\\n",
    "             \"article_id=D5FA11B0-61FD-4217-AAF7-1FC44D897DA1\"\n",
    "\n",
    "start_page_parse_result = urlparse(start_page)\n",
    "start_page_parse_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "page = requests.get(start_page_parse_result.geturl())\n",
    "tree = html.fromstring(page.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found links for 355 film pages.\n"
     ]
    }
   ],
   "source": [
    "# Now extract the information we want\n",
    "\n",
    "# Get a list of URLs to the VIFF pages of each film\n",
    "film_elements = tree.xpath('/html/body//div[@class=\"article-container main-article-body\"]//div/a')\n",
    "film_page_links = {\n",
    "    el.text.strip(): el.attrib['href'] for el in film_elements\n",
    "}\n",
    "\n",
    "print(\"Found links for %d film pages.\" % len(film_page_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_link_to_url(link, scheme=start_page_parse_result.scheme,\n",
    "                        netloc=start_page_parse_result.netloc,\n",
    "                        path=start_page_parse_result.path):\n",
    "    \n",
    "    \"\"\"Converts a relative link such as 'default.asp' into an absolute url\n",
    "    such as 'https://www.viff.org/Online/default.asp' using the scheme,\n",
    "    netloc, and path specified.\"\"\"\n",
    "    \n",
    "    parse_result = urlparse(link)\n",
    "\n",
    "    if parse_result.netloc is '':\n",
    "        parse_result = parse_result._replace(netloc=netloc)\n",
    "    if parse_result.scheme is '':\n",
    "        parse_result = parse_result._replace(scheme=scheme)\n",
    "    if start_page_parse_result.path.endswith(parse_result.path):\n",
    "        parse_result = parse_result._replace(path=path)\n",
    "    \n",
    "    return parse_result.geturl()\n",
    "\n",
    "def get_page_lxml_tree(url):\n",
    "    \"\"\"Request page from url and convert contents into lxml tree\"\"\"\n",
    "\n",
    "    page = requests.get(url)\n",
    "\n",
    "    return html.fromstring(page.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from 10 films found in data file.\n"
     ]
    }
   ],
   "source": [
    "# Prepare dictionary to collect the film information\n",
    "year = pd.datetime.now().date().year\n",
    "data_filename = \"viff_data_%d.pickle\" % year\n",
    "try:\n",
    "    with open(\"viff_data_%d.pickle\" % year, 'rb') as f:\n",
    "        films = pickle.load(f)\n",
    "except:\n",
    "    films = {}\n",
    "else:\n",
    "    print(\"Data from %d films found in data file.\" % len(films))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading information on each film\n",
      "   10: At War -> 5 info records found.\n",
      "WARNING: Getting title for film 'Ãga' failed -> Skipped.\n",
      "   11: Where -> 4 info records found.\n",
      "   12: The Favourite -> 4 info records found.\n",
      "   13: The Hymns of Muscovy -> 5 info records found.\n",
      "   14: ante mis ojos -> 5 info records found.\n",
      "   15: The Front Runner -> 4 info records found.\n",
      "   16: Prawn -> 4 info records found.\n",
      "   17: Colette -> 4 info records found.\n",
      "   18: Daughter of Mine -> 5 info records found.\n",
      "   19: Birds of Passage -> 5 info records found.\n"
     ]
    }
   ],
   "source": [
    "# Now load each film page and extract the information we are looking for\n",
    "\n",
    "# Set number of films to parse each time (None for all)\n",
    "batch = 10\n",
    "\n",
    "already_done = list(films.keys())\n",
    "total_count = len(films)\n",
    "\n",
    "print(\"\\nReading information on each film\")\n",
    "\n",
    "for title, link in film_page_links.items():\n",
    "    \n",
    "    # Skip if already in films dictionary\n",
    "    if title in already_done:\n",
    "        continue\n",
    "\n",
    "    # Convert link to a complete url\n",
    "    page_url = convert_link_to_url(link)\n",
    "\n",
    "    # Request page and convert contents to lxml tree\n",
    "    film_page_tree = get_page_lxml_tree(page_url)\n",
    "\n",
    "    # Get film title (xpath: '//*[@class=\"movie-title\"]')\n",
    "    try:\n",
    "        film_title = film_page_tree.find('.//h1[@class=\"movie-title\"]').text.strip()\n",
    "    except:\n",
    "        print(\"WARNING: Getting title for film '%s' failed -> Skipped.\" % film_title)\n",
    "        continue\n",
    "\n",
    "    # Get film information\n",
    "    film_info = {}\n",
    "    try:\n",
    "        film_information_elements = film_page_tree.find('.//div[@class=\"movie-information\"]').getchildren()\n",
    "    except:\n",
    "        print(\"WARNING: Getting information for film '%s' failed.\" % film_title)\n",
    "    else:\n",
    "        # Put film information into a dictionary\n",
    "        labels = ['Director', 'Year:', 'Country of Origin:', \n",
    "                  'Running Time:', 'Language:']\n",
    "        for e in film_information_elements:\n",
    "            text = e.text_content().strip()\n",
    "\n",
    "            for label in labels:\n",
    "                if text[0:len(label)] == label:\n",
    "\n",
    "                    # remove ':' if there is one\n",
    "                    if label[-1] == ':':\n",
    "                        key = label[0:-1]\n",
    "                    else:\n",
    "                        key = label\n",
    "\n",
    "                    film_info[key] = text[text.find(label) + len(label):].strip()\n",
    "\n",
    "    # Get film description\n",
    "    try:\n",
    "        film_description = film_page_tree.find('.//div[@class=\"movie-description\"]').text_content().strip()\n",
    "    except:\n",
    "        film_description = None\n",
    "        print(\"WARNING: Description for film '%s' missing.\" % film_title)\n",
    "\n",
    "    print(\" %4d: %s -> %d info records found.\" % (total_count, film_title, len(film_info)))\n",
    "\n",
    "    films[film_title] = {\n",
    "        'Information': film_info,\n",
    "        'Description': film_description\n",
    "    }\n",
    "\n",
    "    total_count += 1\n",
    "    if batch:\n",
    "        batch -= 1\n",
    "        if batch < 1:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Information on 20 films now acquired.\n",
      "Results saved to file 'viff_data_2018.pickle'.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nInformation on %d films now acquired.\" % len(films))\n",
    "\n",
    "# Now save the film records to an excel file\n",
    "with open(data_filename, 'wb') as f:\n",
    "    pickle.dump(films, f)\n",
    "print(\"Results saved to file '%s'.\" % data_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Move data into pandas dataframe\n",
    "columns = set()\n",
    "data = dict()\n",
    "\n",
    "for film in films: \n",
    "    for key in films[film]['Information']:\n",
    "        columns = columns.union([key])\n",
    "\n",
    "for column in columns:\n",
    "    data[column] = []\n",
    "\n",
    "data['Description'] = []\n",
    "data['Title'] = []\n",
    "\n",
    "for film in films:\n",
    "    data['Title'].append(film)\n",
    "    for column in columns:\n",
    "        try:\n",
    "            data[column].append(films[film]['Information'][column])\n",
    "        except KeyError:\n",
    "            data[column].append(np.nan)\n",
    "    data['Description'].append(films[film]['Description'])\n",
    "\n",
    "# Dataframe from data\n",
    "df = pd.DataFrame(data=data)\n",
    "\n",
    "# Re-order so title column is first\n",
    "new_columns = list(df.columns.values)\n",
    "new_columns.remove('Title')\n",
    "new_columns = ['Title'] + new_columns\n",
    "df = df[new_columns]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results to Excel file 'VIFF2018_all.xls'\n"
     ]
    }
   ],
   "source": [
    "excel_output_filename = \"VIFF%d_all.xls\" % year\n",
    "print(\"Saving results to Excel file '%s'\" % excel_output_filename)\n",
    "\n",
    "# Save to excel\n",
    "df.to_excel(excel_output_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative script using Selenium to load page information\n",
    "\n",
    "This version is able to read movie screening times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " -------------- viff_scrape2.py --------------\n",
      "\n",
      "Scrape information on this year's films from 'https://www.viff.org'\n",
      "Launching Selenium web-page driver...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ParseResult(scheme='https', netloc='www.viff.org', path='/Online/default.asp', params='', query='doWork::WScontent::loadArticle=Load&BOparam::WScontent::loadArticle::article_id=D5FA11B0-61FD-4217-AAF7-1FC44D897DA1', fragment='')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Python script to scrape film information from the\n",
    "# Vancouver International Film Festival (VIFF) website\n",
    "# using Selenium\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import pickle\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "try:\n",
    "    # For Python 3\n",
    "    from urllib.parse import urlparse\n",
    "except ImportError:\n",
    "    # For Python 2\n",
    "    from urlparse import urlparse\n",
    "\n",
    "print(\"\\n -------------- viff_scrape2.py --------------\\n\")\n",
    "\n",
    "print(\"Scrape information on this year's films from 'https://www.viff.org'\")\n",
    "year = pd.datetime.now().date().year\n",
    "\n",
    "print(\"Launching Selenium web-page driver...\")\n",
    "driver = webdriver.Safari()  # Other options include Chrome, Firefox\n",
    "\n",
    "# Enter the base URL where the A-Z list of films is here (from\n",
    "# the viff.org home page find the 'Search by Title' option):\n",
    "start_page_url = \"https://www.viff.org/Online/default.asp?\" \\\n",
    "                 \"doWork::WScontent::loadArticle=Load&BOparam::\" \\\n",
    "                 \"WScontent::loadArticle::\" \\\n",
    "                 \"article_id=D5FA11B0-61FD-4217-AAF7-1FC44D897DA1\"\n",
    "\n",
    "start_page_parse_result = urlparse(start_page_url)\n",
    "start_page_parse_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening Viff film list web-page...\n"
     ]
    }
   ],
   "source": [
    "print(\"Opening Viff film list web-page...\")\n",
    "# Open webpage in driver\n",
    "driver.get(start_page_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found links for 355 film pages.\n"
     ]
    }
   ],
   "source": [
    "# Now extract the information we want\n",
    "\n",
    "# Get a list of URLs to the VIFF pages of each film\n",
    "xp = '/html/body//div[@class=\"article-container main-article-body\"]//div/a'\n",
    "film_link_elements = driver.find_elements_by_xpath(xp)\n",
    "film_page_links = [\n",
    "    {'text': el.text.strip(), \"href\": el.get_attribute('href')} \n",
    "    for el in film_link_elements\n",
    "]\n",
    "print(\"Found links for %d film pages.\" % len(film_page_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_link_to_url(link, scheme=start_page_parse_result.scheme,\n",
    "                        netloc=start_page_parse_result.netloc,\n",
    "                        path=start_page_parse_result.path):\n",
    "    \"\"\"Converts a relative link such as 'default.asp' into an absolute url\n",
    "    such as 'https://www.viff.org/Online/default.asp' using the scheme,\n",
    "    netloc, and path specified.\"\"\"\n",
    "    \n",
    "    parse_result = urlparse(link)\n",
    "\n",
    "    if parse_result.netloc is '':\n",
    "        parse_result = parse_result._replace(netloc=netloc)\n",
    "    if parse_result.scheme is '':\n",
    "        parse_result = parse_result._replace(scheme=scheme)\n",
    "    if start_page_parse_result.path.endswith(parse_result.path):\n",
    "        parse_result = parse_result._replace(path=path)\n",
    "    \n",
    "    return parse_result.geturl()\n",
    "\n",
    "def get_page_lxml_tree(url):\n",
    "    \"\"\"Request page from url and convert contents into lxml tree\"\"\"\n",
    "\n",
    "    page = requests.get(url)\n",
    "\n",
    "    return html.fromstring(page.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_film_details(driver, max_screenings=5):\n",
    "    \"\"\"Reads film information from Selenium web-driver.\"\"\"\n",
    "    \n",
    "    # Get movie title\n",
    "    xp = './/h1[@class=\"movie-title\"]'\n",
    "    try:\n",
    "        film_title = driver.find_element_by_xpath(xp).text.strip()\n",
    "    except:\n",
    "        print(\"WARNING: Title for film '%s' not found on page \" \\\n",
    "              \"-> skipped.\" % film_title)\n",
    "        return None\n",
    "\n",
    "    # Get film information\n",
    "    film_info = {}\n",
    "    xp = './/div[@class=\"movie-information\"]/*'\n",
    "    try:\n",
    "        film_info_elements = driver.find_elements_by_xpath(xp)\n",
    "    except:\n",
    "        print(\"WARNING: Getting information for film '%s' failed.\" % film_title)\n",
    "    else:\n",
    "        # Put film information into a dictionary\n",
    "        labels = ['Director', 'Year:', 'Country of Origin:', \n",
    "                  'Running Time:', 'Language:']\n",
    "        for e in film_info_elements:\n",
    "            text = e.text.strip()\n",
    "\n",
    "            for label in labels:\n",
    "                if text[0:len(label)] == label:\n",
    "\n",
    "                    # remove ':' if there is one\n",
    "                    if label[-1] == ':':\n",
    "                        key = label[0:-1]\n",
    "                    else:\n",
    "                        key = label\n",
    "\n",
    "                    film_info[key] = text[text.find(label) + len(label):].strip()\n",
    "    \n",
    "    # Get film description\n",
    "    xp = '//div[@class=\"movie-description\"]'\n",
    "    try:\n",
    "        film_description = driver.find_element_by_xpath(xp).text.strip()\n",
    "    except:\n",
    "        film_description = None\n",
    "        print(\"WARNING: Description for film '%s' missing.\" % film_title)\n",
    "    \n",
    "    # Get screening times\n",
    "    xp = '//div[@class=\"movie-tickets\"]/div[@name=\"avWidget\"]' \\\n",
    "         '//div[@class=\"item-description result-box-item-details\"]'\n",
    "    search_result_elements = driver.find_elements_by_xpath(xp)\n",
    "    screenings = []\n",
    "    \n",
    "    if len(search_result_elements) > max_screenings:\n",
    "        print(\"WARNING: Film '%s' has more than %d screening times.\"\n",
    "              \"Only 4 will be saved\" % (film_title, max_screenings))\n",
    "\n",
    "    for element in search_result_elements:\n",
    "        \n",
    "        start_date_string = element.find_element_by_class_name('item-start-date') \\\n",
    "                            .find_element_by_class_name('start-date').text\n",
    "        venue = element.find_element_by_class_name('item-venue').text\n",
    "        \n",
    "        screenings.append({\n",
    "                'Start date': pd.to_datetime(start_date_string),\n",
    "                'Venue': venue\n",
    "            })\n",
    "    \n",
    "    film_details = {\n",
    "        'Title': film_title,\n",
    "        'Information': film_info,\n",
    "        'Description': film_description,\n",
    "        'Screenings': screenings\n",
    "    }\n",
    "    \n",
    "    return film_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from 355 films found in data file.\n"
     ]
    }
   ],
   "source": [
    "# Prepare dictionary to collect the film information\n",
    "year = pd.datetime.now().date().year\n",
    "data_filename = \"viff_data_%d_2.pickle\" % year\n",
    "try:\n",
    "    with open(data_filename, 'rb') as f:\n",
    "        films = pickle.load(f)\n",
    "except:\n",
    "    films = {}\n",
    "else:\n",
    "    print(\"Data from %d films found in data file.\" % len(films))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading each film page to extract information...\n"
     ]
    }
   ],
   "source": [
    "# ----------------- MAIN LOOP -----------------\n",
    "print(\"loading each film page to extract information...\")\n",
    "\n",
    "# Set number of films to parse each time (None for all)\n",
    "batch = 10\n",
    "\n",
    "already_done = list(films.keys())\n",
    "total_count = len(films)\n",
    "max_screenings = 3\n",
    "\n",
    "film_page_links[2]\n",
    "for page_link in film_page_links:\n",
    "    \n",
    "    title, link = page_link['text'], page_link['href']\n",
    "\n",
    "    # Skip if already in films dictionary\n",
    "    if title in already_done:\n",
    "        continue\n",
    "\n",
    "    # Convert link to a complete url\n",
    "    page_url = convert_link_to_url(link)\n",
    "    \n",
    "    # Get film information\n",
    "    driver.get(page_url)\n",
    "    film_details = get_film_details(driver)\n",
    "    \n",
    "    if film_details is None:\n",
    "        # Skip this film\n",
    "        continue\n",
    "\n",
    "    if film_details['Title'] != title:\n",
    "        print(\"WARNING: Title for film '%s' does not match link.\" % title)\n",
    "    \n",
    "    films[title] = film_details\n",
    "    print(\" %4d: %s\" % (total_count, film_details['Title']))\n",
    "\n",
    "    total_count += 1\n",
    "    if batch:\n",
    "        batch -= 1\n",
    "        if batch < 1:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Information on 355 films now acquired.\n",
      "Results saved to file 'viff_data_2018_2.pickle'.\n",
      "Closing Selenium web-page driver...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nInformation on %d films now acquired.\" % len(films))\n",
    "\n",
    "# Now save the film records to an excel file\n",
    "with open(data_filename, 'wb') as f:\n",
    "    pickle.dump(films, f)\n",
    "print(\"Results saved to file '%s'.\" % data_filename)\n",
    "\n",
    "print(\"Closing Selenium web-page driver...\")\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Country of Origin</th>\n",
       "      <th>Description</th>\n",
       "      <th>Director</th>\n",
       "      <th>Language</th>\n",
       "      <th>Running Time</th>\n",
       "      <th>Screening date 1</th>\n",
       "      <th>Screening date 2</th>\n",
       "      <th>Screening date 3</th>\n",
       "      <th>Screening venue 1</th>\n",
       "      <th>Screening venue 2</th>\n",
       "      <th>Screening venue 3</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Unearthing. In Conversation</td>\n",
       "      <td>Austria</td>\n",
       "      <td>Traversing the intersection of theory and art ...</td>\n",
       "      <td>Belinda Kazeem-Kamiński</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13 mins</td>\n",
       "      <td>2018-10-02 18:15:00</td>\n",
       "      <td>2018-10-07 14:00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>Vancity Theatre</td>\n",
       "      <td>Vancity Theatre</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Microhabitat</td>\n",
       "      <td>South Korea</td>\n",
       "      <td>If your rent goes up, do you give up 1) cigare...</td>\n",
       "      <td>Jeon Gowoon</td>\n",
       "      <td>In Korean with English subtitles</td>\n",
       "      <td>106 mins</td>\n",
       "      <td>2018-09-30 18:30:00</td>\n",
       "      <td>2018-10-02 16:00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>International Village 9</td>\n",
       "      <td>International Village 9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Winter Flies</td>\n",
       "      <td>Czech Republic</td>\n",
       "      <td>Stand By Me meets Y Tu Mamá También on a stret...</td>\n",
       "      <td>Olmo Omerzu</td>\n",
       "      <td>In Czech with English subtitles</td>\n",
       "      <td>85 mins</td>\n",
       "      <td>2018-10-05 18:45:00</td>\n",
       "      <td>2018-10-11 10:45:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>SFU Goldcorp</td>\n",
       "      <td>SFU Goldcorp</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>First Generation</td>\n",
       "      <td>USA</td>\n",
       "      <td>A teen Vietnamese-American girl, confused by m...</td>\n",
       "      <td>Jeannie Nguyen</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9 mins</td>\n",
       "      <td>2018-10-04 11:00:00</td>\n",
       "      <td>2018-10-07 18:00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>International Village 9</td>\n",
       "      <td>International Village 9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Carmine Street Guitars</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Nestled in Greenwich Village, Carmine Street G...</td>\n",
       "      <td>Ron Mann</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80 mins</td>\n",
       "      <td>2018-10-02 18:15:00</td>\n",
       "      <td>2018-10-07 16:30:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>Rio Theatre</td>\n",
       "      <td>SFU Goldcorp</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Title Country of Origin  \\\n",
       "0  Unearthing. In Conversation           Austria   \n",
       "1                 Microhabitat       South Korea   \n",
       "2                 Winter Flies    Czech Republic   \n",
       "3             First Generation               USA   \n",
       "4       Carmine Street Guitars            Canada   \n",
       "\n",
       "                                         Description                 Director  \\\n",
       "0  Traversing the intersection of theory and art ...  Belinda Kazeem-Kamiński   \n",
       "1  If your rent goes up, do you give up 1) cigare...              Jeon Gowoon   \n",
       "2  Stand By Me meets Y Tu Mamá También on a stret...              Olmo Omerzu   \n",
       "3  A teen Vietnamese-American girl, confused by m...           Jeannie Nguyen   \n",
       "4  Nestled in Greenwich Village, Carmine Street G...                 Ron Mann   \n",
       "\n",
       "                           Language Running Time    Screening date 1  \\\n",
       "0                               NaN      13 mins 2018-10-02 18:15:00   \n",
       "1  In Korean with English subtitles     106 mins 2018-09-30 18:30:00   \n",
       "2   In Czech with English subtitles      85 mins 2018-10-05 18:45:00   \n",
       "3                               NaN       9 mins 2018-10-04 11:00:00   \n",
       "4                               NaN      80 mins 2018-10-02 18:15:00   \n",
       "\n",
       "     Screening date 2 Screening date 3        Screening venue 1  \\\n",
       "0 2018-10-07 14:00:00              NaT          Vancity Theatre   \n",
       "1 2018-10-02 16:00:00              NaT  International Village 9   \n",
       "2 2018-10-11 10:45:00              NaT             SFU Goldcorp   \n",
       "3 2018-10-07 18:00:00              NaT  International Village 9   \n",
       "4 2018-10-07 16:30:00              NaT              Rio Theatre   \n",
       "\n",
       "         Screening venue 2 Screening venue 3  Year  \n",
       "0          Vancity Theatre               NaN  2017  \n",
       "1  International Village 9               NaN  2017  \n",
       "2             SFU Goldcorp               NaN  2018  \n",
       "3  International Village 9               NaN  2017  \n",
       "4             SFU Goldcorp               NaN  2018  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Move data into pandas dataframe\n",
    "info_cols = set()\n",
    "data = dict()\n",
    "\n",
    "for film in films: \n",
    "    for key in films[film]['Information']:\n",
    "        info_cols = info_cols.union([key])\n",
    "\n",
    "max_screenings = max([len(details['Screenings']) for \n",
    "                      details in films.values()])\n",
    "\n",
    "date_cols = set([\"Screening date %d\" % (i + 1) for i in range(max_screenings)])\n",
    "venue_cols = set([\"Screening venue %d\" % (i + 1) for i in range(max_screenings)])\n",
    "\n",
    "# Add an empty list to dictionary for each series expected\n",
    "for column in info_cols.union(date_cols).union(venue_cols):\n",
    "    data[column] = []\n",
    "data['Description'] = []\n",
    "data['Title'] = []\n",
    "\n",
    "for film in films:\n",
    "    data['Title'].append(film)\n",
    "    for col in info_cols:\n",
    "        try:\n",
    "            data[col].append(films[film]['Information'][col])\n",
    "        except KeyError:\n",
    "            data[col].append(np.nan)\n",
    "    data['Description'].append(films[film]['Description'])\n",
    "    for i, cols in enumerate(zip(date_cols, venue_cols)):\n",
    "        try:\n",
    "            data[cols[0]].append(films[film]['Screenings'][i]['Start date'])\n",
    "            data[cols[1]].append(films[film]['Screenings'][i]['Venue'])\n",
    "        except IndexError:\n",
    "            data[cols[0]].append(np.nan)\n",
    "            data[cols[1]].append(np.nan)\n",
    "\n",
    "# Dataframe from data\n",
    "df = pd.DataFrame(data=data)\n",
    "\n",
    "# Re-order so title column is first\n",
    "new_columns = list(df.columns.values)\n",
    "new_columns.remove('Title')\n",
    "new_columns = ['Title'] + new_columns\n",
    "df = df[new_columns]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results to Excel file 'VIFF2018_all_with_dates.xls'\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "excel_output_filename = \"VIFF%d_all_with_dates.xls\" % year\n",
    "print(\"Saving results to Excel file '%s'\" % excel_output_filename)\n",
    "\n",
    "# Save to excel\n",
    "df.to_excel(excel_output_filename)\n",
    "\n",
    "print(\"Finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do some fun analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "355"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(films)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "title_text = ' '.join([film for film in films])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "982"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = title_text.split()\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_tally = {}\n",
    "for word in words:\n",
    "    word_lower = word.lower().strip('()-_:[]{}& ')\n",
    "    if word_lower:\n",
    "        word_tally[word_lower] = word_tally.get(word_lower, 0) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the, of, a, and, in, is, at, an, what, on, la, to, 1, 2, 3\n"
     ]
    }
   ],
   "source": [
    "exclude_words = ['the', 'of', 'a', 'and', 'in', 'is', 'at', 'an', 'what', 'on', 'la', 'to', '1', '2', '3']\n",
    "\n",
    "for word in exclude_words:\n",
    "    if word in word_tally:\n",
    "        del(word_tally[word])\n",
    "\n",
    "print(', '.join(exclude_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "my                7\n",
       "waves             6\n",
       "war               6\n",
       "shock             6\n",
       "house             5\n",
       "room              4\n",
       "life              4\n",
       "street            4\n",
       "under             4\n",
       "you               4\n",
       "trilogy           4\n",
       "quantification    4\n",
       "flor              3\n",
       "wild              3\n",
       "happy             3\n",
       "story             3\n",
       "up                3\n",
       "dreaming          3\n",
       "part              3\n",
       "song              3\n",
       "dtype: int64"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(word_tally).sort_values(ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my, waves, war, shock, house, room, life, street, under, you\n"
     ]
    }
   ],
   "source": [
    "top10 = pd.Series(word_tally).sort_values(ascending=False).head(10).index.values.tolist()\n",
    "print(', '.join(top10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'happy'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bytes(pd.Series(word_tally).sort_values(ascending=False).head(20).index.values.tolist()[14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "my:\n",
      " The Whistleblower of My Lai\n",
      " Shock Waves: Diary of My Mind\n",
      " Inside My Heart\n",
      " My Life Is a Joke\n",
      " In My Room (Germany\n",
      " In My Room (Israel\n",
      " My Clayey Conception\n",
      "\n",
      "waves:\n",
      " Shock Waves: Diary of My Mind\n",
      " Shock Waves: The Valley\n",
      " Shock Waves: First Name Mathieu\n",
      " Shock Waves: Sirius\n",
      " Shock Waves: Program 1 & 2\n",
      " Shock Waves: Program 3 & 4\n",
      "\n",
      "war:\n",
      " At War\n",
      " Theatre of War\n",
      " Woman at War\n",
      " A Private War\n",
      " Memoir of War\n",
      " Cold War\n",
      "\n",
      "shock:\n",
      " Shock Waves: Diary of My Mind\n",
      " Shock Waves: The Valley\n",
      " Shock Waves: First Name Mathieu\n",
      " Shock Waves: Sirius\n",
      " Shock Waves: Program 1 & 2\n",
      " Shock Waves: Program 3 & 4\n",
      "\n",
      "house:\n",
      " A Dreaming House\n",
      " Open House\n",
      " Julio Iglesias's House\n",
      " The Wolf House\n",
      " The House That Jack Built\n",
      "\n",
      "room:\n",
      " A Room with a Coconut View\n",
      " Maybe if It Were a Nice Room\n",
      " In My Room (Germany\n",
      " In My Room (Israel\n",
      "\n",
      "life:\n",
      " Another Day of Life\n",
      " Bergman - A Year in a Life\n",
      " This Mountain Life\n",
      " My Life Is a Joke\n",
      "\n",
      "street:\n",
      " Carmine Street Guitars\n",
      " The Beetle at the End of the Street\n",
      " From Across the Street and Through Two Sets of Windows\n",
      " No. 1 Chung Ying Street\n",
      "\n",
      "under:\n",
      " Under the Viaduct\n",
      " Dreaming Under Capitalism + The Washing Society\n",
      " Dreaming Under Capitalism\n",
      " Under the Silver Lake\n",
      "\n",
      "you:\n",
      " You Don't Know Me\n",
      " Can You Ever Forgive Me?\n",
      " Reel Youth Film Festival\n",
      " What You Gonna Do When the World's On Fire?\n",
      " Open Your Eyes\n",
      " The Image You Missed\n"
     ]
    }
   ],
   "source": [
    "for word in top10:\n",
    "    print(\"\\n%s:\" % word)\n",
    "    print(' ' + '\\n '.join([film.strip('()-_:[]{} ') for film in films if (word in film.lower())]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Unearthing. In Conversation',\n",
       " u'Microhabitat',\n",
       " u'Winter Flies',\n",
       " u'First Generation',\n",
       " u'Carmine Street Guitars',\n",
       " u'Piazza Vittorio',\n",
       " u'Another Day of Life',\n",
       " u'Parallel',\n",
       " u'Sharkwater Extinction',\n",
       " u'LHB']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "films.keys()[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Trying to fix encoding/decoding issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'//*[@id=\"avWidget_F265B724-7400-4814-97B9-70ADA2D69B87_IDY1ISUEB1M3LBBE3WJI5ZR4D4SNOIU0LAGBK2J5K4VSTH1D2QFACO\"]/div/div[1]/form/div'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'//*[@id=\"avWidget_F265B724-7400-4814-97B9-70ADA2D69B87_IDY1ISUEB1M3LBBE3WJI5ZR4D4SNOIU0LAGBK2J5K4VSTH1D2QFACO\"]/div/div[1]/form/div'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'Dealing with what comes naturally isn\\u2019t easy, especially for animals. Five such beasts gather to discuss their inner angst in a group therapy session.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desc = films['Animal Behaviour']['Description']\n",
    "desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dealing with what comes naturally isn\\xe2\\x80\\x99t easy, especially for animals. Five such beasts gather to discuss their inner angst in a group therapy session.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desc.encode('utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "isn`t easy\n"
     ]
    }
   ],
   "source": [
    "print('isn`t easy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'Dealing with what comes naturally isn\\u2019t easy, especially for animals. Five such beasts gather to discuss their inner angst in a group therapy session.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'ascii' codec can't encode character u'\\u2019' in position 37: ordinal not in range(128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-36f3020cd450>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdesc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mUnicodeEncodeError\u001b[0m: 'ascii' codec can't encode character u'\\u2019' in position 37: ordinal not in range(128)"
     ]
    }
   ],
   "source": [
    "bytes(desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "desc.decode(\"utf-8\", \"strict\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "link = 'https://www.viff.org/Online/default.asp?BOparam::WScontent::loadArticle::permalink=f30436-animal-behaviour'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "get_page_lxml_tree(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xp = '//div[@id=\"tickets\"]'\n",
    "film_page_tree.xpath(xp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "film_page_tree.xpath(xp)[0].values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xp2 = '//*[@name=\"avWidget\"]'\n",
    "film_page_tree.xpath(xp2)[0].text_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xp3 = '//*[@name=\"avWidget\"]'\n",
    "film_page_tree.xpath(xp3)[0].getchildren()[0].attrib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'//*[@name=\"avWidget\"]/div/div[1]/form/div/div[1]/div[2]/div[2]/span'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dir(film_page_tree.xpath(xp)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (scrapyenv)",
   "language": "python",
   "name": "scrapyenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
